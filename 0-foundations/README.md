# ğŸ§  ML Project 0 â€“ Python & Math Foundations

Hands-on foundations for Machine Learning in pure Python scripts (no notebooks).  
The goal of this project is to **really understand** the building blocks of ML:

- Python for ML:
  - `numpy` â†’ ndarrays, vector & matrix operations, broadcasting
  - `pandas` â†’ (will be used in later projects, e.g. Project 0.1 â€“ EDA)
  - `matplotlib`, `seaborn` â†’ basic visualizations
- Applied Math:
  - Linear algebra â†’ vectors, matrices, matrix multiplication, transpose, inverse, eigenvalues/eigenvectors (intuitive)
  - Probability & statistics â†’ normal distribution, mean, variance, covariance, correlation
  - Calculus â†’ derivative & gradient, gradient descent in 1D

Each file in `src/` is a small, focused script with clean code and comments in English, designed to be readable on GitHub and to generate visual outputs in the `plots/` folder.

---

## ğŸ—‚ Project Structure

```text
ml-project-0-foundations/
    â”œâ”€ src/
    â”‚   â”œâ”€ numpy_basics.py               # vectors, matrices, dot, matmul, transpose, broadcasting
    â”‚   â”œâ”€ linear_algebra_demo.py        # solving Ax=b, determinant, inverse, eigenvalues/vectors
    â”‚   â”œâ”€ stats_basics.py               # mean, variance, covariance, correlation, normal dist + plots
    â”‚   â””â”€ gradient_descent_demo.py      # 1D gradient descent on f(w) = (w - 3)^2
    â”œâ”€ plots/
    â”‚   â”œâ”€ numpy_A_matrix.txt
    â”‚   â”œâ”€ numpy_B_matrix.txt
    â”‚   â”œâ”€ numpy_C_matrix.txt
    â”‚   â”œâ”€ hist_heights.png
    â”‚   â”œâ”€ hist_weights.png
    â”‚   â”œâ”€ scatter_height_vs_weight.png
    â”‚   â””â”€ gradient_descent_1d.png
    â”œâ”€ requirements.txt
    â””â”€ README.md
```

---

## âš™ï¸ Setup

```bash

python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

pip install -r requirements.txt
```

---

## â–¶ï¸ Scripts

### 1. `numpy_basics.py`

**Topics:**

- `ndarray` and `.shape`
- dot product vs manual dot
- matrix multiplication (`@`)
- transpose (`.T`)
- broadcasting (adding a row vector to a matrix)

**Run:**

```bash
python src/numpy_basics.py
```

**Outputs:**

- Prints vector and matrix shapes and dot products
- Saves:
  - `plots/numpy_A_matrix.txt`
  - `plots/numpy_B_matrix.txt`
  - `plots/numpy_C_matrix.txt`

---

### 2. `linear_algebra_demo.py`

**Topics:**

- Solving linear systems \(Ax = b\)
- Determinant and inverse of a 2Ã—2 matrix
- Eigenvalues & eigenvectors, and the relation \(A v = \lambda v\)

**Run:**

```bash
python src/linear_algebra_demo.py
```

**Outputs:**

- Printed solution `x` such that `A @ x â‰ˆ b`
- `det(A)`, `A^{-1}`, and `A @ A^{-1}` (close to identity)
- Eigenvalues and eigenvectors of `A`

---

### 3. `stats_basics.py`

**Topics:**

- Generating random data from a normal distribution
- Mean, variance, standard deviation
- Covariance and correlation between height and weight
- Basic histograms and scatter plot

**Run:**

```bash
python src/stats_basics.py
```

**Visuals (auto-saved):**

#### Histogram of Heights

![Histogram of Heights](plots/hist_heights.png)

#### Histogram of Weights

![Histogram of Weights](plots/hist_weights.png)

#### Height vs Weight Scatter

![Height vs Weight](plots/scatter_height_vs_weight.png)

---

### 4. `gradient_descent_demo.py`

**Topics:**

- 1D objective function: \(f(w) = (w - 3)^2\)
- Analytic derivative: \(f'(w) = 2(w - 3)\)
- Gradient descent update:
  \[
  w_{\text{new}} = w_{\text{old}} - \eta \cdot f'(w_{\text{old}})
  \]

**Run:**

```bash
python src/gradient_descent_demo.py
```

**Visual:**

Gradient descent steps (red dots) moving towards the minimum at \(w = 3\):

![Gradient Descent 1D](plots/gradient_descent_1d.png)

---

## ğŸ§® Applied Math Summary (Intuition)

- **Vector** â†’ an ordered list of numbers (direction + magnitude).
- **Matrix** â†’ a grid of numbers; can represent a linear transformation.
- **Matrix multiplication** â†’ applying one transformation after another.
- **Determinant** â†’ how much the matrix scales area/volume; zero means it collapses space (no inverse).
- **Eigenvector** â†’ a direction that the matrix only stretches, not rotates.  
  **Eigenvalue** â†’ how much it stretches that direction.

- **Mean** â†’ average.
- **Variance** â†’ how wide the data is spread around the mean.
- **Std (standard deviation)** â†’ square root of variance; spread in original units.
- **Covariance** â†’ do two variables move together?
- **Correlation** â†’ normalized covariance, between -1 and 1.

- **Gradient** â†’ vector of partial derivatives; direction of steepest *increase*.
- **Gradient descent** â†’ move in the opposite direction of gradient to *minimize* a function.

---
